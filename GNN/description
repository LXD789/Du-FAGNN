各.py文件主要功能：
一、inits.py（初始化）：
    1.uniform()(统一初始化)：生成元素服从均匀分布的矩阵(元素从中随机取)，返回此tensor类对象。
    2.glorot()(权重初始化)：生成元素服从均匀分布的矩阵(元素从中随机取)，返回此tensor类对象。(Xavier初始化)
    3.zeros()(对偏置项的初始化，生成零矩阵)。返回tensor类对象。
    4.ones()(生成元素全为1的矩阵/张量)。返回tensor类对象。

二、utils.py(各种工具函数的集合文件，包含对矩阵的操作、数据集的标记化和清除字符串处理等)
    1.parse_index_file()(处理索引文件):去除文件中字符串头尾字符。返回新字符串列表(该列表包含整个文件中的新字符串)。

    2.sample_mask()(生成掩码)：生成全零数组后，将某些位置的元素置为1，最后返回元素为bool类型的数组以表示掩码。

    3.load_data()(从文件中加载并解析、提取(分离)输入数据)：
        (1)将文件的序列化内容进行反序列化解析。
        (2)分别将训练集、测试集和验证集的邻接矩阵和特征向量提取出来单独存放为列表，并将他们转换为数组。
        (3)将“带标签的训练集的独热标签”转换为数组;将allx中有标记的那部分训练集数据的标签的
            “从‘y的元素个数’到‘ally的元素个数’”这部分数据转换为数组；将“测试集的独热标签”转换为数组。
        返回(2)(3)中的9个数组。

    4.sparse_to_tuple()(将稀疏矩阵转换为元组表示形式)，返回元组类型的稀疏矩阵。

    5.coo_to_tuple():将coo_matrix(对角存储矩阵)类型转换为tuple(元组)类型。返回矩阵的转置、矩阵的内容和矩阵的形状。

    6.preprocess_features()(行归一化特征矩阵并转换为元组表示)：将特征矩阵组成的总矩阵中的每个元素(即每个特征矩阵)进行填充，
        并重新赋值给当前的总特征矩阵，最后将总特征矩阵转换为数组类型并返回。

    7.normalize_adj()(对称归一化邻接矩阵)：将矩阵的每一行元素相加得到的新矩阵进行-0.5次方操作、降为一维操作，
      然后进行对角化操作，最后将其与原矩阵进行点乘、转置再点乘，返回此结果(此结果为数组)。

    8.preprocess_adj()(简单GCN模型的邻接矩阵预处理并转换为元组表示)：调用函数(normalize_adj()，对称归一化邻接矩阵)。
      对邻接矩阵进行归一化操作和填充操作，并将新元素重新赋值给原邻接矩阵；另外还对矩阵进行掩码操作。
      最终返回新邻接矩阵(数组)和掩码矩阵(数组)。

    9.construct_feed_dict()(构造提要字典)：给使用placeholder(占位符)创建出来的tensor赋值。

    10.chebyshev_polynomials()(计算Chebyshev多项式直至k。 返回稀疏矩阵的列表（元组表示）。)

    11.loadWord2Vec()(读取词向量)：读取文件后，将文件中的每行文本进行去除头尾字符串和分割操作，将每行的第一个词
       添加进vocab列表，其余的词在赋值给vector列表后进行类型转换(string——>float)，然后将vector添加进embed列表，
       再将word_vector_map字典中键为“每行第一个元素”对应的值赋值为vector。最后返回vocab列表、embed列表和word_vector_map字典。

    12.clean_str()(除SST外，所有数据集的标记化/字符串清除。)

    13.clean_str_sst()(SST数据集的标记化/字符串清除。)

三、remove_words.py(去除停用词)：
备注：从utils.py文件中引入了clean_str(), clean_str_sst(), loadWord2Vec()三个函数。
    无函数，但总体功能上分为5部分：
    1.读取某数据集，加载、显示stop_words(停用词)，并设置语言为英文。
    2.打开数据集文件并读取，去除每行的头尾字符后进行解码。
    3.计算每个词的词频。
    4.将分词后的词列表进行清洗(即将符合要求的词筛选出来)。
    5.计算文件中每行文本包含的词语个数，并输出含元素最长行、含元素最短行和平均行。

四、layers.py(对层的操作以及四个层类的定义——基础层、稠密层、图层和图读出层)
   1.get_layer_uid()(获取层ID)

   2.sparse_dropout()(丢弃稀疏张量)：
        （1）将元素保留概率矩阵(张量)取出
        （2）将元素保留概率矩阵与新随机生成的噪音矩阵(张量)相加
        （3）将（2）中的新矩阵(张量)中的元素向下取整后转换为bool型
        （4）将此函数中的输入矩阵(张量)x与(3)中生成的bool型矩阵相结合，bool型矩阵中true的位置对应的x中的元素会被保留
        （5）返回值为“(4)中生成的新矩阵”与“(1)的元素保留概率矩阵的倒数”相乘的结果

   3.sparse_dense_matmul_batch()(稀疏-稠密-乘积):
    备注：内有函数map_function()，其功能是对稀疏张量进行重塑并将其与稠密张量做乘积。
        (1)创建一个元组，其内容为从0开始到“该函数第一个参数”止的增量为1的数字序列，和该函数第二个参数。
        (2)调用map_function()，使用tf.map_fn()映射从维度0的“elem”((1)里创建的元组)解压缩的张量列表(即运行tf.map_fn()，
           elems((1)里创建的元组)中所有的元素都执行一次map_function()，返回值为elems中每个张量进行map_function()返回的张量或张量序列)

   4.dot():
        （1）若参数sparse为True，则定义res，将其赋值为sparse_dense_matmul_batch()(3中的函数)的返回值。
        （2）若参数sparse为False，则定义res，将其赋值为两个参数矩阵的乘积。
        返回（1）或（2）的结果（可能是张量类型的矩阵）

   5.gru_unit():带有3D张量输入的GRU单元。用来学习单词节点的嵌入情况。
        (1)消息传递
            1）产生丢弃后的张量
            2）计算两张量的乘积(其中一个为dropout后的张量)
        (2)更新门
        (3)复位门
        (4)更新嵌入

  类：
   6.基础层类Layer。 为所有图层对象定义基本API。
     属性：
      (1)name：定义层的可变范围
      (2)logging：开启/关闭Tensorflow直方图记录
      (3)vars:
      (4)sparse_inputs:稀疏输入？
     方法(函数)：
      (1)_call(inputs)：定义层的计算图
      (2)__call__(inputs)：_call()函数的装饰器
      (3)_log_vars()：记录所有变量

   7.稠密层类Dense(继承基础层)。
     属性：
      (1)dropout:丢弃标志(maybe)
      (2)act：
      (3)sparse_inputs:稀疏输入？
      (4)featureless:
      (5)bias:偏置项
      (6)num_features_nonzero:为稀疏丢弃而设定的帮助变量(辅助变量)
      (7)vars:
    方法：
      (1)__init__():为属性赋值；在赋值过程中调用了:
        1)inits.py中的glorot()函数(初始化)
        2)inits.py中的zero()函数(生成零矩阵)
      (2)_call():分为4部分：
        1)dropout:调用当前,py文件中的sparse_dropout()函数，或tf.nn.dropout()(调用哪个函数取决于某一bool型参数值)，
                  并将函数返回值赋值给x(输入)。
        2)transform:调用当前.py文件中的dot()函数，其返回值为当前输出。相当于"y=wx+b"中的"wx"。
        3)bias:在2)的输出基础上添加偏置项(加法运算)，将其作为当前输出。相当于"y=wx+b"中的"+b"。
        4)将3)的输出作为某激活函数的输入，最终返回激活函数的输出。

   8.图层GraphLayer(继承基础层)。
    属性：
      (1)dropout:
      (2)act:
      (3)support:
      (4)sparse_inputs:
      (5)featureless:
      (6)bias:
      (7)mask:
      (8)steps:
      (9)num_features_nonzero:
      (10)vars:
      (11)logging:
    方法：
      (1)__init__():为属性赋值，在赋值过程中调用了inits.py中的glorot()函数(初始化)和zeros()(生成零矩阵)函数。
      (2)_call()：分为3部分：
        1)dropout：调用当前.py文件中的sparse_dropout()函数，或tf.nn.dropout()(调用哪个函数取决于某一bool型参数值)，
                   并将函数返回值赋值给x(输入)。
        2)encode inputs(编码输入)：调用当前.py文件中的dot()函数，x重新赋值为其返回值(dot()的输入为原x，结果为新x)。
                                   相当于"y=wx+b"。然后，将新x输入激活函数后所得的结果与类对象的mask属性相乘，
                                   将结果赋值给output。
        3)convolve(卷积)：调用当前.py文件中的gru_unit()函数，2)里的原output作为其中一个输入，输出新output。

   9.读出层ReadoutLayer(继承基础层)。
    方法：
      (1)__init__():为属性赋值，在赋值过程中调用了inits.py中的glorot()函数(初始化)和zeros()(生成零矩阵)函数。
      (2)_call():分为4部分：
        1)soft attention(软注意):对应文章中的f1(软注意权重)，相当于"sigmoid(wx+b)";对应文章中的f2(非线性特征变换)，
                                此处的act在实际应用中是tanh激活函数，相当于"tanh(wx+b)"。
        2)对类对象的mask属性进行竖向的求和操作；mask全体减一后与接近于正无穷的数做乘积。
        3)graph summation图总结：将1)中的f1和f2相乘并进行掩码；
                                将文章中的“hG=(h1+h2+...+hn)/n+maxpooling(h1,h2,...,hn)”进行实现。
        4)classification分类：将3)的输出作为输入，执行“y=wx+b”线性变换。


五、metrics.py(定义指标(损失函数和精确度))
    1.softmax_cross_entropy()(带掩码的softmax交叉熵损失)：计算交叉熵，返回交叉熵张量的平均值。

    2.accuracy()(带掩码的精确度):
      (1)比较预测结果与真实标签是否相等。
      (2)将比较后的返回值(bool型数组)转换为float型数组。
      (3)计算总体精确度的平均值。

六、build_graph.py(构造图并生成文件)
注：此处的“文档”指的是数据集文件中的一行文本。即每个文档即为一行文本。
    1.加载预训练的词嵌入
    2.加载文件列表：将“id+训练集/测试集+标签”添加进相应列表
    3.加载行(hang)文本
    4.(序列)映射(序列映射指通过相应函数对序列进行操作)和随机排序
    5.建立语料库词汇
    6.初始化词汇表外词嵌入
    7.建立标签列表
    8.选择90%训练集

    定义图函数：build_graph()
        （1）将数据集中的一行文本取出后：
            1）对其进行截断处理
            2）计算截断处理后的文本列表所含词语个数
            3）对文本列表中的词语进行去重操作（去除重复词汇）
            4）计算去重操作后的文本词汇列表所含词汇个数
            5）将词语与其id做映射
        （2）滑动窗口的代码实现
        （3）对（2）中滑动窗口里共同出现的词语对，计算其出现的次数(频数)，并将此作为边的权重
        （4）产生x的邻接矩阵
        （5）产生x的特征矩阵
        （6）产生独热标签

    9.建立图(调用build_graph())
    10.统计
    11.转储对象(储存对象)

七、model.py(模型——基础模型类、多层感知机类和图神经网络类)
    1.基础模型类Model。
    属性：
    name:
    logging:
    vars:(字典类)
    placeholders:(字典类)
    layers:(列表类)
    activations:(列表类)
    inputs:
    outputs:
    embeddings:
    loss:
    accuracy:
    optimizer:
    opt_op:

    方法：
    （1）__init__()：类属性赋值
    （2）_build()：类的私有属性，要求本类的子类务必实现此方法。若子类没有实现此方法，则使用raise报出错误。
    （3）build()：_build()函数的装饰器
        1）建立顺序层模型
        2）存储模型变量以便于访问
        3）建立(评估)指标
    （4）predict()：空函数
    （5）_loss()：盲猜此函数用来计算损失。类的私有属性，要求本类的子类务必实现此方法。
                若子类没有实现此方法，则使用raise报出错误。
    （6）_accuracy()：盲猜此函数用来计算精度。类的私有属性，要求本类的子类务必实现此方法。
                    若子类没有实现此方法，则使用raise报出错误。
    （7）save()：保存模型函数
    （8）load()；加载模型函数

    2.多层感知器类MLP（集成基础模型类）。
    方法：
    （1）__init__()：类属性赋值
    （2）_build()：父类要求子类必须实现的方法。向类的layer列表属性中添加稠密层和读出层。
    （3）_loss()：父类要求子类必须实现的方法。
        1）权重衰变损失
        2）交叉熵错误
    （4）_accuracy()：父类要求子类必须实现的方法。调用了metrics.py中的accuracy()。
    （5）predict()：调用softmax()函数，返回其返回值。

    3.图神经网络类GNN（继承基础模型类）。
    方法：
        （1）__init__():类属性赋值
        （2）_build()：父类要求子类必须实现的方法。向类的layer列表中添加图层和读出层。
        （3）_loss()：父类要求子类必须实现的方法。
        （4）_accuracy()：父类要求子类必须实现的方法。调用了metrics.py中的accuracy()。
        （5）predict()：调用softmax()函数，返回其返回值。

八、train.py(训练)
    1.设置
    2.加载数据
    3.预处理：分别对训练集、验证集和测试集的邻接矩阵和特征矩阵进行预处理
    4.对所用模型进行匹配
    5.定义占位符
    6.生成模型
    7.初始化Session对象
    8.定义模型评估函数
    9.初始化变量
    10.训练模型
        （1）训练步骤。根据命令行输入的batch_size来定义每个batch。
            1）构造提要字典（将占位符赋值）
            2）更新训练损失和训练精度
            3）训练结束后计算平均损失和平均精度
        （2）验证
        （3）测试。调用evaluate()函数，其返回测试集上的参数。最后更新最好精度和最少损失。
        （4）打印结果
    11.打印最好结果